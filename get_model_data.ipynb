{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-equation coveriance error term\n",
    "# in each case error terms are gaussian\n",
    "# what are they picking up\n",
    "# unobservable errors are correlated\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import plot\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yfinance as yf\n",
    "from arch.univariate import ARCH, GARCH\n",
    "from arch.univariate import ARX\n",
    "\n",
    "local_drive = Path('C:/Users/torin/OneDrive')\n",
    "databonds = Path('Desktop/Thesis/data/bonds/model')\n",
    "datamacro = Path('Desktop/Thesis/data/macro')\n",
    "\n",
    "def normalize_periods(df):\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.set_index(['DATE']).resample('MS').mean()\n",
    "    return df\n",
    "\n",
    "def calc_difference(df, col_names, new_col_name):\n",
    "    df[new_col_name] = df[col_names[0]] - df[col_names[1]]\n",
    "    return df\n",
    "\n",
    "def calc_ratio(df, col_names, new_col_name):\n",
    "    df[new_col_name] = df[col_names[0]]/df[col_names[1]]\n",
    "    return df\n",
    "\n",
    "def get_fx():\n",
    "    eur = get_yf('EURUSD')[['Date','Close']]\n",
    "    eur.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    eur['Close'] = pd.to_numeric(eur['Close'])\n",
    "    eur['DATE'] = eur['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    eur = normalize_periods(eur)\n",
    "    eur.columns = ['eur']\n",
    "\n",
    "    brl = get_yf('BRLUSD')[['Date','Close']]\n",
    "    brl.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    brl['Close'] = pd.to_numeric(brl['Close'])\n",
    "    brl['DATE'] = brl['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    brl = normalize_periods(brl)\n",
    "    brl.columns = ['brl']\n",
    "\n",
    "    ch = get_yf('CLPUSD')[['Date','Close']]\n",
    "    ch.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    ch['Close'] = pd.to_numeric(ch['Close'])\n",
    "    ch['DATE'] = ch['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    ch = normalize_periods(ch)\n",
    "    ch.columns = ['clp']\n",
    "    ch['clp'] = 1/ch['clp']\n",
    "\n",
    "    fx = pd.concat([eur, brl, ch], axis = 1)\n",
    "\n",
    "    return fx\n",
    "\n",
    "def get_yf(ticker):    \n",
    "    yfpath = Path('C:/Users/torin/OneDrive/Desktop/Thesis/data/macro/yf')\n",
    "    data = pd.read_csv(yfpath / str(ticker + '.csv'))\n",
    "    return data\n",
    "\n",
    "def get_yahoo_finance_series(ticker, name):\n",
    "    data = yf.Ticker(ticker)\n",
    "    hist = data.history(period=\"25y\")\n",
    "    hist = hist.reset_index()\n",
    "    hist['Date'] = pd.to_datetime(hist['Date'])\n",
    "    hist.to_csv(\"C:/Users/torin/OneDrive/Desktop/Thesis/data/macro/yf\" +'/' + name + '.csv')\n",
    "    return hist\n",
    "\n",
    "def convert_to_usd(col_to_convert, original_currency, df):\n",
    "    fx = get_fx().resample('MS').mean().to_dict()\n",
    "    df['fx'] = df.index.map(fx[original_currency])\n",
    "    df[col_to_convert] = df[col_to_convert]*df['fx']\n",
    "    del df['fx']\n",
    "    return df\n",
    "\n",
    "tenors=[2,3,4,5,6,7,8,10,12,15,20,25,30]\n",
    "\n",
    "monhtly_sector_spreads = {}\n",
    "for tenor in tenors:\n",
    "    temp = pd.read_csv(local_drive / databonds / Path('IT_BR') / Path('monthly_spreads_' + str(tenor) + '.csv'))\n",
    "    temp.rename({'date':'DATE'}, axis =1, inplace=True)\n",
    "    temp = normalize_periods(temp)\n",
    "    monhtly_sector_spreads[tenor] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\torin\\AppData\\Local\\Temp\\ipykernel_12996\\3675667053.py:284: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  br['DATE'] = pd.to_datetime(br['DATE'] )\n"
     ]
    }
   ],
   "source": [
    "########### macro variables ###############\n",
    "\n",
    "    \n",
    "def get_gdp(growth, lag):\n",
    "    # EXOG VARIABLES\n",
    "    # gd\n",
    "\n",
    "    gdplevel = pd.read_csv(local_drive / datamacro / Path('model')  / Path('oecd_weekly_gdp_levels.csv') )\n",
    "    gdppercapitalevel = pd.read_csv(local_drive / datamacro / Path('model')  / Path('oecd_weekly_gdp_level_percapita.csv') )\n",
    "\n",
    "    del gdplevel['Unnamed: 0']\n",
    "    del gdppercapitalevel['Unnamed: 0']\n",
    "\n",
    "    gdplevel.rename({'date':'DATE'}, axis =1, inplace=True)\n",
    "    gdppercapitalevel.rename({'date':'DATE'}, axis =1, inplace=True)\n",
    "\n",
    "    gdplevel = gdplevel.pivot(index=['DATE'], values = ['GDP'], columns=['region']).reset_index()\n",
    "    gdplevel.columns = ['DATE', 'Brazil_GDP','Chile_GDP',  'Italy_GDP', 'United States_GDP']\n",
    "\n",
    "    gdppercapitalevel = gdppercapitalevel.pivot(index=['DATE'], values = ['GDP'], columns=['region']).reset_index()\n",
    "    gdppercapitalevel.columns = ['DATE', 'Brazil_GDPcapita', 'Chile_GDPcapita','Italy_GDPcapita', 'United States_GDPcapita']\n",
    "\n",
    "    gdplevel['DATE'], gdppercapitalevel['DATE'] = pd.to_datetime(gdplevel['DATE']), pd.to_datetime(gdppercapitalevel['DATE'] )\n",
    "\n",
    "    gdplevel,gdppercapitalevel = normalize_periods(gdplevel), normalize_periods(gdppercapitalevel)\n",
    "\n",
    "    if growth == True:\n",
    "        for col in gdplevel:\n",
    "            gdplevel[col] = (gdplevel[col]/(gdplevel[col].shift(12)) - 1)*100\n",
    "\n",
    "        for col in gdppercapitalevel:\n",
    "            gdppercapitalevel[col] = (gdppercapitalevel[col]/(gdppercapitalevel[col].shift(12)) - 1)*100\n",
    "\n",
    "        gdplevel, gdppercapitalevel = gdplevel.dropna(), gdppercapitalevel.dropna()\n",
    "\n",
    "    gdplevel,gdppercapitalevel  = calc_difference(gdplevel,['Brazil_GDP', 'Italy_GDP'], 'diff_br_it_gdp'), calc_difference(gdppercapitalevel,['Brazil_GDPcapita', 'Italy_GDPcapita'], 'diff_br_it_gdp_per_cp')\n",
    "    gdplevel,gdppercapitalevel  = calc_difference(gdplevel,['Brazil_GDP', 'United States_GDP'], 'diff_br_us_gdp'), calc_difference(gdppercapitalevel,['Brazil_GDPcapita', 'United States_GDPcapita'], 'diff_br_us_gdp_per_cp')\n",
    "    gdplevel,gdppercapitalevel  = calc_difference(gdplevel,['Italy_GDP', 'United States_GDP'], 'diff_it_us_gdp'), calc_difference(gdppercapitalevel,['Italy_GDPcapita', 'United States_GDPcapita'], 'diff_it_us_gdp_per_cp')\n",
    "    gdplevel,gdppercapitalevel  = calc_difference(gdplevel,['Chile_GDP', 'United States_GDP'], 'diff_ch_us_gdp'), calc_difference(gdppercapitalevel,['Chile_GDPcapita', 'United States_GDPcapita'], 'diff_ch_us_gdp_per_cp')\n",
    "\n",
    "    gdplevel,gdppercapitalevel  = calc_ratio(gdplevel,['Brazil_GDP', 'United States_GDP'], 'ratio_br_us_gdp'), calc_difference(gdppercapitalevel,['Brazil_GDPcapita', 'United States_GDPcapita'], 'ratio_br_us_gdp_per_cp')\n",
    "    gdplevel,gdppercapitalevel  = calc_ratio(gdplevel,['Italy_GDP', 'United States_GDP'], 'ratio_it_us_gdp'), calc_difference(gdppercapitalevel,['Italy_GDPcapita', 'United States_GDPcapita'], 'ratio_it_us_gdp_per_cp')\n",
    "\n",
    "    gdp = pd.concat([gdplevel, gdppercapitalevel ], axis = 1).dropna()\n",
    "\n",
    "    if lag == True:\n",
    "        for col in gdp.columns:\n",
    "            gdp[col] = gdp[col].shift(3)\n",
    "    \n",
    "    gdp = gdp.dropna()\n",
    "\n",
    "    return gdp\n",
    "\n",
    "def get_inflation():\n",
    "    # EXOG VARIABLES\n",
    "    # inflation\n",
    "\n",
    "    br_inflation = pd.read_csv(local_drive / datamacro / Path('model')  / Path('br_inflation.csv') )\n",
    "    it_inflation = pd.read_csv(local_drive / datamacro / Path('model')  / Path('it_inflation.csv') )\n",
    "\n",
    "    br_inflation['br_inflation_mom'] = br_inflation['IPCA'].pct_change()*100\n",
    "    br_inflation['br_inflation_yoy'] = (br_inflation['IPCA'].div(br_inflation['IPCA'].shift(12)) - 1)*100\n",
    "\n",
    "    del br_inflation['IPCA']\n",
    "    br_inflation = br_inflation.dropna()\n",
    "\n",
    "\n",
    "    it_inflation['it_inflation_mom'] = it_inflation['HICP - All Items'].pct_change()*100\n",
    "    it_inflation['it_inflation_yoy'] = (it_inflation['HICP - All Items'].div(it_inflation['HICP - All Items'].shift(12)) - 1)*100\n",
    "    del it_inflation['HICP - All Items']\n",
    "    it_inflation.rename({'Date':'DATE'}, axis =1, inplace=True)\n",
    "    it_inflation = it_inflation.dropna()\n",
    "\n",
    "    it_inflation['DATE'], br_inflation['DATE'] = pd.to_datetime(it_inflation['DATE']), pd.to_datetime(br_inflation['DATE'] )\n",
    "\n",
    "    it_inflation,br_inflation = normalize_periods(it_inflation), normalize_periods(br_inflation)\n",
    "\n",
    "    inflation = pd.concat([br_inflation, it_inflation ], axis = 1).dropna()\n",
    "\n",
    "    inflation = calc_difference(inflation, ['br_inflation_yoy', 'it_inflation_yoy'], 'diff_br_it_inflation_yoy')\n",
    "    inflation = calc_difference(inflation, ['br_inflation_mom', 'it_inflation_mom'], 'diff_br_it_inflation_mom')\n",
    "\n",
    "    return inflation\n",
    "\n",
    "def get_debt(gdplevel):\n",
    "    # EXOG VARIABLES\n",
    "    # debt\n",
    "    br_debt = pd.read_csv(local_drive / datamacro / Path('model')  / Path('br_debt.csv') )\n",
    "    br_debt_nominal = br_debt.pivot(index=['DATE'], values = ['VALUE (R$)'], columns=['category']).reset_index().dropna(axis = 1, how = 'all')\n",
    "    br_debt_nominal.columns = br_debt_nominal.columns.droplevel(0)\n",
    "    br_debt_nominal.rename(columns={ br_debt_nominal.columns[0]: \"DATE\" }, inplace = True)\n",
    "\n",
    "    br_debt_perc = br_debt.pivot(index=['DATE'], values = ['VALUE ((% PIB))'], columns=['category']).reset_index().dropna(axis = 1, how = 'all')\n",
    "    br_debt_perc.columns = br_debt_perc.columns.droplevel(0)\n",
    "    br_debt_perc.rename(columns={ br_debt_perc.columns[0]: \"DATE\" }, inplace = True)\n",
    "\n",
    "    br_debt_nominal['DATE'], br_debt_perc['DATE'] = pd.to_datetime(br_debt_nominal['DATE']), pd.to_datetime(br_debt_perc['DATE'])\n",
    "    br_debt_nominal, br_debt_perc = normalize_periods(br_debt_nominal),normalize_periods(br_debt_perc)\n",
    "\n",
    "    br_debt_nominal = convert_to_usd('DLSP - Dívida Fiscal Líquida','brl',br_debt_nominal)\n",
    "    br_debt_nominal = convert_to_usd('DLSP - dívida externa líquida - total','brl',br_debt_nominal)\n",
    "    br_debt_nominal = convert_to_usd('DLSP - dívida interna líquida','brl',br_debt_nominal)\n",
    "\n",
    "\n",
    "    it_debt = pd.read_csv(local_drive / datamacro/ Path('model') /Path('bank_of_italy_statistics.csv'))\n",
    "    it_debt.rename({'Observation date':'DATE'}, axis =1, inplace=True)\n",
    "    it_debt = it_debt[['DATE','General Government: gross debt']]\n",
    "    it_debt['DATE'] = pd.to_datetime(it_debt['DATE'] )\n",
    "    it_debt = normalize_periods(it_debt)\n",
    "    it_debt = convert_to_usd('General Government: gross debt','eur',it_debt)\n",
    "\n",
    "    debt = pd.concat([br_debt_nominal, br_debt_perc , it_debt], axis = 1).dropna()\n",
    "\n",
    "    debt = debt[['DLSP - Dívida Fiscal Líquida', 'DLSP - Dívida Fiscal Líquida (% of pib)', 'General Government: gross debt']]\n",
    "\n",
    "    debt['it_gdp_temp'] = pd.to_datetime(debt.index.copy()).map(gdplevel[['Italy_GDP']].to_dict()['Italy_GDP'])\n",
    "    debt['it_debt_perc_gdp'] = (debt['General Government: gross debt']/debt['it_gdp_temp'])*100\n",
    "    debt = debt.dropna()\n",
    "    del debt['it_gdp_temp']\n",
    "    debt = calc_difference(debt, ['DLSP - Dívida Fiscal Líquida (% of pib)', 'it_debt_perc_gdp'], 'diff_br_it_debt_perc_gdp')\n",
    "    debt = calc_difference(debt, ['DLSP - Dívida Fiscal Líquida', 'General Government: gross debt'], 'diff_br_it_debt')\n",
    "    return debt\n",
    "\n",
    "def get_deficit(gdplevel):\n",
    "\n",
    "    # EXOG VARIABLES\n",
    "    # debt servicing and deficits\n",
    "\n",
    "    it_deficit = pd.read_csv(local_drive / datamacro / Path('model')  / Path('it_deficit_trend.csv'))\n",
    "    it_deficit.rename({'Observation date':'DATE'}, axis =1, inplace=True)\n",
    "\n",
    "    br_deficit = pd.read_csv(local_drive / datamacro / Path('model')  / Path('br_deficit.csv'))\n",
    "\n",
    "    deficit_code_map = {'BM12_NFGFJNNS12': 'NFSP - governo federal e Banco Central - juros nominais', \n",
    "    'BM12_NFGFNNAS12': 'NFSP - governo federal e Banco Central - nominal', \n",
    "    'BM12_NFGFNYS12': 'NFSP - governo federal e Banco Central - primário'}\n",
    "\n",
    "    br_deficit['CODE'] = br_deficit['CODE'].map(deficit_code_map)\n",
    "\n",
    "    br_deficit_nominal = br_deficit.pivot(index=['DATE'], values = ['VALUE (R$)'], columns=['CODE']).reset_index()\n",
    "    br_deficit_nominal = br_deficit_nominal.dropna(how = 'all', axis = 1)\n",
    "    br_deficit_nominal.columns = br_deficit_nominal.columns.droplevel(0)\n",
    "    br_deficit_nominal.rename(columns={ br_deficit_nominal.columns[0]: \"DATE\" }, inplace = True)\n",
    "\n",
    "    br_deficit_perc = br_deficit.pivot(index=['DATE'], values = ['VALUE ((% PIB))'], columns=['CODE']).reset_index()\n",
    "    br_deficit_perc = br_deficit_perc.dropna(how = 'all', axis = 1)\n",
    "    br_deficit_perc.columns = br_deficit_perc.columns.droplevel(0)\n",
    "    br_deficit_perc.rename(columns={ br_deficit_perc.columns[0]: \"DATE\" }, inplace = True)\n",
    "\n",
    "    br_deficit_nominal['DATE'], br_deficit_perc['DATE'], it_deficit['DATE'] = pd.to_datetime(br_deficit_nominal['DATE']), pd.to_datetime(br_deficit_perc['DATE']), pd.to_datetime(it_deficit['DATE'] )\n",
    "\n",
    "    it_deficit, br_deficit_nominal, br_deficit_perc = normalize_periods(it_deficit),normalize_periods(br_deficit_nominal), normalize_periods(br_deficit_perc)\n",
    "\n",
    "    br_deficit_nominal = convert_to_usd('NFSP - governo federal e Banco Central - nominal','brl',br_deficit_nominal)\n",
    "    br_deficit_nominal = convert_to_usd('NFSP - governo federal e Banco Central - juros nominais','brl',br_deficit_nominal)\n",
    "    it_deficit = convert_to_usd('trend - state deficit','eur',it_deficit)\n",
    "\n",
    "    deficit = pd.concat([it_deficit, br_deficit_nominal,br_deficit_perc ], axis = 1).dropna()\n",
    "    deficit['it_gdp_temp'] = pd.to_datetime(deficit.index.copy()).map(gdplevel[['Italy_GDP']].to_dict()['Italy_GDP'])\n",
    "    deficit['it_deficit_perc_gdp'] = (deficit['trend - state deficit']/deficit['it_gdp_temp'])*100\n",
    "    deficit.dropna()\n",
    "    del deficit['it_gdp_temp']\n",
    "\n",
    "    deficit = calc_difference(deficit, ['NFSP - governo federal e Banco Central - nominal', 'trend - state deficit'], 'diff_br_it_deficit')\n",
    "    deficit = calc_difference(deficit, ['NFSP - governo federal e Banco Central - primário', 'it_deficit_perc_gdp'], 'diff_br_it_deficit_perc_gdp')\n",
    "\n",
    "    return deficit\n",
    "\n",
    "def get_foreign_reserves(gdplevel):\n",
    "    br_fr = pd.read_csv(local_drive / datamacro / Path('model')  / Path('br_foreign_reserves.csv') )\n",
    "    code_map = {'BM12_RES12': 'Foreign Reserves'}\n",
    "    br_fr['CODE'] = br_fr['CODE'].map(code_map)\n",
    "    br_fr = br_fr[['DATE','VALUE (US$)']]\n",
    "    br_fr = normalize_periods(br_fr)\n",
    "    br_fr.columns = ['br_foreign_reserves']\n",
    "\n",
    "    br_fr['br_gdp_temp'] = pd.to_datetime(br_fr.index.copy()).map(gdplevel[['Brazil_GDP']].to_dict()['Brazil_GDP'])\n",
    "    br_fr['br_foreign_reserves_gdp'] = (br_fr['br_foreign_reserves']/br_fr['br_gdp_temp'])*100\n",
    "    del br_fr['br_gdp_temp']\n",
    "\n",
    "    # Official reserve assets - Foreign currency reserves (in convertible foreign currencies)\n",
    "    it_fr = pd.read_csv(local_drive / datamacro/ Path('model') /Path('bank_of_italy_statistics.csv'))\n",
    "    it_fr.rename({'Observation date':'DATE'}, axis =1, inplace=True)\n",
    "    it_fr = it_fr[['DATE','Official reserve assets - Foreign currency reserves (in convertible foreign currencies)']]\n",
    "    it_fr = normalize_periods(it_fr)\n",
    "    it_fr.columns = ['it_foreign_reserves']\n",
    "\n",
    "    it_fr['it_gdp_temp'] = pd.to_datetime(it_fr.index.copy()).map(gdplevel[['Italy_GDP']].to_dict()['Italy_GDP'])\n",
    "    it_fr['it_foreign_reserves_gdp'] = (it_fr['it_foreign_reserves']/it_fr['it_gdp_temp'])*100\n",
    "    del it_fr['it_gdp_temp']\n",
    "\n",
    "    fr = pd.concat([br_fr, it_fr], axis = 1)\n",
    "\n",
    "    fr = calc_difference(fr, ['br_foreign_reserves','it_foreign_reserves'], 'diff_br_it_foreign_reserves')\n",
    "    fr = calc_difference(fr, ['br_foreign_reserves_gdp','it_foreign_reserves_gdp'], 'diff_br_it_foreign_reserves_gdp')\n",
    "\n",
    "    return fr\n",
    "\n",
    "def get_fx():\n",
    "    eur = get_yf('EURUSD')[['Date','Close']]\n",
    "    eur.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    eur['Close'] = pd.to_numeric(eur['Close'])\n",
    "    eur['DATE'] = eur['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    eur = normalize_periods(eur)\n",
    "    eur.columns = ['eur']\n",
    "\n",
    "    brl = get_yf('BRLUSD')[['Date','Close']]\n",
    "    brl.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    brl['Close'] = pd.to_numeric(brl['Close'])\n",
    "    brl['DATE'] = brl['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    brl = normalize_periods(brl)\n",
    "    brl.columns = ['brl']\n",
    "\n",
    "    ch = get_yf('CLPUSD')[['Date','Close']]\n",
    "    ch.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    ch['Close'] = pd.to_numeric(ch['Close'])\n",
    "    ch['DATE'] = ch['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    ch = normalize_periods(ch)\n",
    "    ch.columns = ['clp']\n",
    "\n",
    "    fx = pd.concat([eur, brl, ch], axis = 1)\n",
    "\n",
    "    return fx\n",
    "\n",
    "\n",
    "def get_balance_of_trade(gdplevel):\n",
    "\n",
    "    balancetradebr = pd.read_csv(local_drive / datamacro/ Path('model') /Path('br_balancetrade.csv'))\n",
    "    balancetradeit  = pd.read_csv(local_drive / datamacro/ Path('model') /Path('it_balance_of_trade.csv'))\n",
    "\n",
    "    balancetradeit.rename({'Select time':'DATE'}, axis = 1, inplace=True)\n",
    "\n",
    "    balancetradebr['DATE'] = pd.to_datetime(balancetradebr['DATE'] )\n",
    "    balancetradeit['DATE'] = pd.to_datetime(balancetradeit['DATE'] )\n",
    "\n",
    "    balancetradebr = balancetradebr.pivot(index = 'DATE', columns='Cat',values='VALUE (US$)').reset_index()\n",
    "\n",
    "    balancetradebr = normalize_periods(balancetradebr)\n",
    "    balancetradeit = normalize_periods(balancetradeit)\n",
    "\n",
    "    balancetradeit = convert_to_usd('export - value (millions of euros)','eur',balancetradeit)\n",
    "    balancetradeit = convert_to_usd('import - value (millions of euros)','eur',balancetradeit)\n",
    "    balancetradeit = convert_to_usd('trade balance - value (millions of euros)','eur',balancetradeit)\n",
    "\n",
    "    balancetradeit.rename({'export - value (millions of euros)':'it export - value (millions of dollars)'}, axis = 1, inplace=True)\n",
    "    balancetradeit.rename({'import - value (millions of euros)':'it import - value (millions of dollars)'}, axis = 1, inplace=True)\n",
    "    balancetradeit.rename({'trade balance - value (millions of euros)':'it trade balance - value (millions of dollars)'}, axis = 1, inplace=True)\n",
    "    \n",
    "    balancetrade = pd.concat([balancetradebr,balancetradeit], axis = 1)\n",
    "\n",
    "    balancetrade['it_gdp_temp'] = pd.to_datetime(balancetrade.index.copy()).map(gdplevel[['Italy_GDP']].to_dict()['Italy_GDP'])\n",
    "    balancetrade['it_trade_bal_gdp'] = (balancetrade['it trade balance - value (millions of dollars)']/balancetrade['it_gdp_temp'])*100\n",
    "    balancetrade = balancetrade.dropna()\n",
    "    del balancetrade['it_gdp_temp']\n",
    "\n",
    "    balancetrade['br_gdp_temp'] = pd.to_datetime(balancetrade.index.copy()).map(gdplevel[['Brazil_GDP']].to_dict()['Brazil_GDP'])\n",
    "    balancetrade['br_trade_bal_gdp'] = (balancetrade['balance_of_trade']/balancetrade['br_gdp_temp'])*100\n",
    "    balancetrade = balancetrade.dropna()\n",
    "    del balancetrade['br_gdp_temp']\n",
    "\n",
    "    return balancetrade\n",
    "\n",
    "def debt_composition():\n",
    "    it = pd.read_csv(local_drive / datamacro/ Path('model') /Path('bank_of_italy_statistics.csv'))\n",
    "    it.rename({'Observation date':'DATE'}, axis =1, inplace=True)\n",
    "    \n",
    "    it_debt_holders = ['General Government: gross debt held by other residents (share)',\n",
    "       'General Government: gross debt held by other monetary financial institutions (share)',\n",
    "       'General Government: gross debt held by other financial institutions (share)',\n",
    "       'General Government: gross debt held by central bank (share)',\n",
    "       'General Government: gross debt held by non-residents (share)',]\n",
    "     \n",
    "    it = it[['DATE'] + it_debt_holders]\n",
    "    it = normalize_periods(it)\n",
    "    it = it[['General Government: gross debt held by non-residents (share)',\n",
    "             'General Government: gross debt held by other monetary financial institutions (share)', \n",
    "             'General Government: gross debt held by central bank (share)']]\n",
    "    it.columns = ['it_non_resident_share', 'it_other_mon_share', 'it_cb_share']\n",
    "\n",
    "    it['it_central_banks_share'] = it['it_other_mon_share'] + it['it_cb_share']\n",
    "\n",
    "    br = pd.read_csv(local_drive / datamacro/ Path('model') /Path('br_debt_holders.csv'))\n",
    "    br['DATE'] = br['DATE'].str.replace('/','-')\n",
    "    br['DATE'] = pd.to_datetime(br['DATE'] )\n",
    "    br = br.set_index(['DATE'])\n",
    "    for col in br.columns:\n",
    "        br[col] = pd.to_numeric(br[col].astype(str).str.replace(',',''))\n",
    "\n",
    "    br = br.reset_index()\n",
    "    br = normalize_periods(br)\n",
    "    br = br[['Non-Resident Total (share)']]\n",
    "    br.columns = ['br_non_resident_share']\n",
    "\n",
    "    brcbshare = pd.read_csv((local_drive / datamacro/ Path('model') /Path('br_cb_share.csv')))\n",
    "    brcbshare.columns = ['Date', 'br_cb_share']\n",
    "    brcbshare['Date'] = pd.to_datetime(brcbshare['Date'])\n",
    "    brcbshare = brcbshare.set_index(['Date']).to_dict()['br_cb_share']\n",
    "\n",
    "    br['br_central_banks_share'] = br.index.map(brcbshare)\n",
    "\n",
    "    non_res = pd.concat([br, it], axis = 1)\n",
    "\n",
    "    non_res = calc_difference(non_res, ['br_non_resident_share', 'it_non_resident_share'], 'diff_non_resident_share')\n",
    "    non_res = calc_difference(non_res, ['br_central_banks_share', 'it_central_banks_share'], 'diff_central_banks_share')\n",
    "\n",
    "    return non_res.dropna()\n",
    "\n",
    "\n",
    "def exchange_rate_vol(exchange_rate):\n",
    "\n",
    "    for col in exchange_rate.columns:\n",
    "        ar = ARX(100 * np.log(exchange_rate[col].dropna()), lags=[1, 3, 12])\n",
    "        ar.volatility = ARCH(p=5)\n",
    "        res = ar.fit(update_freq=0, disp=\"off\")\n",
    "        vol = pd.DataFrame(res.conditional_volatility.dropna())\n",
    "        vol.columns = [col + '_vol']\n",
    "\n",
    "        exchange_rate[col + '_vol'] = vol[col + '_vol']\n",
    "\n",
    "    return exchange_rate\n",
    "\n",
    "gdp = get_gdp(growth=False, lag = False)\n",
    "gdp_growth = get_gdp(growth=True, lag = False)\n",
    "gdp_growth = gdp_growth[['Brazil_GDP','Italy_GDP']]\n",
    "gdp_growth.columns = ['brazil_gdp_growth','italy_gdp_growth']\n",
    "\n",
    "inflation = get_inflation()\n",
    "debt = get_debt(gdp)\n",
    "deficit = get_deficit(gdp)\n",
    "tradebal = get_balance_of_trade(gdp)\n",
    "exchange_rate = get_fx()\n",
    "exchange_rate = exchange_rate_vol(exchange_rate)\n",
    "foreign_reserves = get_foreign_reserves(gdp)\n",
    "non_residents = debt_composition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\torin\\AppData\\Local\\Temp\\ipykernel_12996\\3004767102.py:19: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['DATE'] = pd.to_datetime(df['DATE'])\n",
      "C:\\Users\\torin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arch\\univariate\\base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
      "estimating the model parameters. The scale of y is 8.247e+04. Parameter\n",
      "estimation work better when this value is between 1 and 1000. The recommended\n",
      "rescaling is 0.1 * y.\n",
      "\n",
      "This warning can be disabled by either rescaling y before initializing the\n",
      "model or by setting rescale=False.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "########### external/market variables ###############\n",
    "\n",
    "def get_vix():\n",
    "    vix = get_yf('VIX')[['Date','Close']]\n",
    "    vix.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    vix['Close'] = pd.to_numeric(vix['Close'])\n",
    "    vix['DATE'] = vix['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    vix = normalize_periods(vix)\n",
    "    vix.columns = ['vix']\n",
    "    return vix\n",
    "\n",
    "def get_us3m():\n",
    "    us3m = get_yf('DTB3')[['Date','Close']]\n",
    "    us3m.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    us3m['Close'] = pd.to_numeric(us3m['Close'], errors='coerce')\n",
    "    us3m = us3m.dropna()\n",
    "    us3m['DATE'] = us3m['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    us3m = normalize_periods(us3m)\n",
    "    us3m.columns = ['us3m']\n",
    "    return us3m\n",
    "\n",
    "def get_gold():\n",
    "    gold = get_yf('GOLD')[['Date','Close']]\n",
    "    gold.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    gold['Close'] = pd.to_numeric(gold['Close'])\n",
    "    gold['DATE'] = gold['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    gold = normalize_periods(gold)\n",
    "    gold.columns = ['gold']\n",
    "    return gold\n",
    "\n",
    "def get_us_high_yield():\n",
    "    ush = get_yf('US_HIGH_YIELD')[['DATE','BAMLH0A0HYM2EY']]\n",
    "    ush.rename({'BAMLH0A0HYM2EY':'us_high_yield'}, axis = 1, inplace=True)\n",
    "    ush['us_high_yield'] = pd.to_numeric(ush['us_high_yield'], errors='coerce')\n",
    "    ush = ush.dropna()\n",
    "    ush['DATE'] = ush['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    ush = normalize_periods(ush)\n",
    "    ush.columns = ['us_high_yield']\n",
    "    return ush\n",
    "\n",
    "def get_commods():\n",
    "    commods = get_yf('DBC')[['Date','Close']]\n",
    "    commods.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    commods['Close'] = pd.to_numeric(commods['Close'])\n",
    "    commods['DATE'] = commods['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    commods = normalize_periods(commods)\n",
    "    commods.columns = ['commods']\n",
    "    return commods\n",
    "\n",
    "def get_fx():\n",
    "    eur = get_yf('EURUSD')[['Date','Close']]\n",
    "    eur.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    eur['Close'] = pd.to_numeric(eur['Close'])\n",
    "    eur['DATE'] = eur['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    eur = normalize_periods(eur)\n",
    "    eur.columns = ['eur']\n",
    "\n",
    "    brl = get_yf('BRLUSD')[['Date','Close']]\n",
    "    brl.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    brl['Close'] = pd.to_numeric(brl['Close'])\n",
    "    brl['DATE'] = brl['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    brl = normalize_periods(brl)\n",
    "    brl.columns = ['brl']\n",
    "\n",
    "    ch = get_yf('CLPUSD')[['Date','Close']]\n",
    "    ch.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    ch['Close'] = pd.to_numeric(ch['Close'])\n",
    "    ch['DATE'] = ch['DATE'].astype(str).apply(lambda x : x[0:10])\n",
    "    ch = normalize_periods(ch)\n",
    "    ch.columns = ['clp']\n",
    "\n",
    "    fx = pd.concat([eur, brl, ch], axis = 1)\n",
    "\n",
    "    return fx\n",
    "\n",
    "def get_contagion():\n",
    "\n",
    "    itvix = pd.read_csv(local_drive / databonds / 'itvix.csv')\n",
    "    brvix = pd.read_csv(local_drive / databonds/ 'brvix.csv')\n",
    "\n",
    "    itvix = itvix[['Date', 'Price']]\n",
    "    brvix = brvix[['Date', 'Price']]\n",
    "\n",
    "    itvix.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "    brvix.rename({'Date':'DATE'}, axis = 1, inplace=True)\n",
    "\n",
    "    itvix['Price'] = pd.to_numeric(itvix['Price'].astype(str).str.replace(',',''))\n",
    "    brvix['Price'] = pd.to_numeric(brvix['Price'].astype(str).str.replace(',',''))\n",
    "\n",
    "    itvix = normalize_periods(itvix)\n",
    "    brvix = normalize_periods(brvix)\n",
    "\n",
    "    itvix.columns = ['it_contagion']\n",
    "    brvix.columns = ['br_contagion']\n",
    "\n",
    "    ar = ARX(brvix, lags=[3])\n",
    "    ar.volatility = ARCH(p=5)\n",
    "    res = ar.fit(update_freq=0, disp=\"off\")\n",
    "    vol = pd.DataFrame(res.conditional_volatility.dropna())\n",
    "    vol.columns = ['br_contagion']\n",
    "\n",
    "    brvix['br_contagion'] = vol['br_contagion']\n",
    "\n",
    "    contagion = pd.concat([itvix, brvix], axis = 1)\n",
    "    return contagion.dropna()\n",
    "\n",
    "vix = get_vix()\n",
    "ushighyield = get_us_high_yield()\n",
    "gold = get_gold()\n",
    "commods = get_commods()\n",
    "us3m = get_us3m()\n",
    "contagion = get_contagion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### merging final dataset ################\n",
    "\n",
    "estimates = {}\n",
    "for k,v in monhtly_sector_spreads.items():\n",
    "    estimates[k] = pd.concat([gdp,\n",
    "                              gdp_growth,\n",
    "                              inflation, \n",
    "                              debt, \n",
    "                              deficit, \n",
    "                              exchange_rate,\n",
    "                              foreign_reserves,\n",
    "                              non_residents,\n",
    "                              tradebal,\n",
    "                              vix, \n",
    "                              ushighyield, \n",
    "                              gold, \n",
    "                              commods,\n",
    "                              us3m,\n",
    "                              contagion,\n",
    "                              v], axis = 1).dropna()\n",
    "\n",
    "def get_model_data(estimates, measure = 'spread'):\n",
    "\n",
    "  estall = pd.concat(estimates).reset_index()\n",
    "  del estall['level_0']\n",
    "  estall['tenor'] = 'tenor_' + estall['tenor'].astype(str).apply(lambda x : str(x)[0:2].replace('.',''))\n",
    "  \n",
    "  if measure == 'spread':\n",
    "    tenorsdf = estall.pivot(columns=['tenor'], values = ['spread_BR_IT'], index = ['DATE'])\n",
    "    tenorsdf.columns = tenorsdf.columns.get_level_values(1)\n",
    "    tenorsdf = tenorsdf.add_prefix('spread_BR_IT_')\n",
    "  \n",
    "  elif measure == 'yield_br':\n",
    "    tenorsdf = estall.pivot(columns=['tenor'], values = ['yield BR'], index = ['DATE'])\n",
    "    tenorsdf.columns = tenorsdf.columns.get_level_values(1)\n",
    "    tenorsdf = tenorsdf.add_prefix('yield_br_')\n",
    "\n",
    "  elif measure == 'yield_it':\n",
    "    tenorsdf = estall.pivot(columns=['tenor'], values = ['yield IT'], index = ['DATE'])\n",
    "    tenorsdf.columns = tenorsdf.columns.get_level_values(1)\n",
    "    tenorsdf = tenorsdf.add_prefix('yield_it_')\n",
    "  \n",
    "  estall = estall.set_index(['DATE']).drop([ \n",
    "      'tenor', \n",
    "      'yield BR', \n",
    "      'yield IT',\n",
    "      'spread_BR_IT'], axis = 1).drop_duplicates().sort_index()\n",
    "\n",
    "  model_data = pd.concat([tenorsdf, \n",
    "                          estall], axis = 1)\n",
    "\n",
    "  model_data = model_data.dropna()\n",
    "\n",
    "  cols_rename = {'DLSP - Dívida Fiscal Líquida':'br_divida_fiscal_liquida',\n",
    "        'DLSP - dívida externa líquida - total': 'br_divida_externa_liquida',\n",
    "        'DLSP - dívida interna líquida': 'br_divida_interna_liquida',\n",
    "        'DLSP - Dívida Fiscal Líquida (% of pib)': 'br_divida_fiscal_liquida_pct_pib',\n",
    "        'DLSP - dívida externa líquida - total (% of pib)': 'br_divida_externa_liquida_pib',\n",
    "        'DLSP - dívida interna líquida (% of pib)': 'br_divida_interna_liquida_pib',\n",
    "        'General Government: gross debt':'it_gross_debt',\n",
    "        'trend - state deficit': 'it_trend_state_deficit',\n",
    "        'NFSP - governo federal e Banco Central - juros nominais': 'br_juros_nominais',\n",
    "        'NFSP - governo federal e Banco Central - nominal': 'br_deficit_nominal',\n",
    "        'NFSP - governo federal e Banco Central - primário': 'br_deficit_primario',\n",
    "        'United States_GDP':'us_gdp',\n",
    "        'United States_GDPcapita':'us_gdp_per_capita',\n",
    "        'Brazil_GDP':'br_gdp',\n",
    "        'Italy_GDP':'it_gdp', \n",
    "        'Italy_GDPcapita':'it_gdp_per_capita',\n",
    "          'Brazil_GDPcapita':'br_gdp_per_capita'}\n",
    "\n",
    "  model_data.rename(cols_rename, axis =1, inplace=True)\n",
    "\n",
    "  return model_data\n",
    "\n",
    "get_model_data(estimates).to_csv(local_drive / databonds/ 'model_data_spread_br_it.csv')\n",
    "get_model_data(estimates, measure = 'yield_it').to_csv(local_drive / databonds/ 'model_data_yield_it.csv')\n",
    "get_model_data(estimates, measure = 'yield_br').to_csv(local_drive / databonds/ 'model_data_yield_br.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(measure = 'spread_br_it'):\n",
    "    if measure == 'spread_br_it':\n",
    "        data = pd.read_csv(local_drive / databonds/'model_data_spread_br_it.csv').set_index(['DATE'])\n",
    "    if measure == 'yield_it':\n",
    "        data = pd.read_csv(local_drive / databonds/'model_data_yield_it.csv').set_index(['DATE'])\n",
    "    if measure == 'yield_br':\n",
    "        data = pd.read_csv(local_drive / databonds/'model_data_yield_br.csv').set_index(['DATE'])\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
